# -*- coding: utf-8 -*-
"""Copy of diss_1st_draft.ipynb

Automatically generated by Colaboratory.

Original file is located at Google Colaboratory
"""

# -*- coding: utf-8 -*-
"""
Created on Sat Jun 22 16:16:08 2019

@author: Lukasz Kozarski

"""

# MOUNTING DRIVEPLOADING FILES

from google.colab import drive

drive.mount('/content/drive')

# INSTALLING IMPORTANT MODULES

!pip install glove_python

from glove import Corpus, Glove

# LOADING PREINSTALLED MODULES

from os import listdir
from os.path import isfile, join

import numpy as np
import pandas as pd
import nltk
from nltk.tokenize import wordpunct_tokenize
from nltk.corpus import brown
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
import gensim

from keras import Sequential
from keras.layers import Dense, Input, Flatten, LSTM
from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout

from sklearn.utils import shuffle

from scipy.stats import ttest_ind as ttest
from scipy.stats import shapiro
from scipy.stats import mannwhitneyu as mwtest

import matplotlib.pyplot as plt

from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_validate
from sklearn.model_selection import GridSearchCV
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import cross_val_score

from numpy.random import seed
from tensorflow import set_random_seed

# Downloading all important packages and setting up seed to ensure
# repeatable results

nltk.download('averaged_perceptron_tagger')
nltk.download('universal_tagset')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('brown')

seed(21031994)
set_random_seed(21031994)

'''
    DEFINITIONS OF SPECIAL FUNCTIONS
'''

def text_preprocess(text = '',
                    special_chars = '!\'\"#$%&()*+,-./:;<=>?@[\]^_`{|}~\t\n'):
  '''
  Function takes two arguments:
      text:
          text to be preprocessed, in a form of one string
      special_chars:
          set of special characters to be removed from the text,
          as default a typical set of special characters is set
          
  Function returns:
      text:
          a string of initial text in lowercase, without the special characters
  '''  
  text = text.lower()
  text = ''.join(char for char in text if (
      char not in special_chars or char == ' '))
  return(text)

def num_of_words(list = [], part_of_speech = 'NOUN'):
  '''
  Function takes two arguments:
      list:
          list of words for the function to tag part of speech in
      part_of_speech:
          a name of part of speech to be tagged in the list of words
          
  Function returns:
      (part_of_speech, num):
          (a tuple of name of the part of speech that was tagged,
           number of cases of the part of speech in the initial list)
  '''  
  part_of_speech = part_of_speech.upper()
  num = 0
  for element in list:
    if element[0] == part_of_speech:
      num = element[1]
  return((part_of_speech, num))


def vect_and_concat(vecMod, input = [], ax = None):
  '''
  Function takes tree arguments:
      vectMod:
          requires a function that returns a vectorized version of the
          elements in input
      input:
          list of elements to be vectorized and concatenated
      ax:
          axis alon which the vector will be concatenated, for further
          information check documentation of np.concatenate()
          
  Function returns:
      output_vector:
          NumPy array of the elements of the input vectorized and concatenated
          along the chozen axix
  '''
  
  output_vector = np.array([])
  for element in input:
    try:
      output_vector = np.concatenate(([output_vector], [vecMod[element]]), ax)
    except:
      pass
  return output_vector


def filter(input_vector = [], filter_size = 1):
  '''
  Function takes two arguments:
      input_vector:
          one-dimentional, iterable variable of numbers
      filter_size:
          size of filter applied on elements of the input vector
          
  Function returns:
      output_vector:
          filtered list of size input_vector/filter_size rounded up
  '''
  
  output_vector = list()
  
  # Number of iterations neccessary to shorten the input to 'size'
  iter_number = len(input_vector) - filter_size + 1
  
  for i in range(iter_number):
    output_vector.append(sum(input_vector[i:i+filter_size])/filter_size)
    
  return output_vector


def short_vect(input_vector, size, filter_size = 1):
  '''
  Function takes three arguments:
      input_vector:
          one-dimentional, iterable variable of numbers to be resized
      size:
          desirable size of the output vector
      filter_size:
          size of filter applied on elements of the input vector
        
  Function returns:
      output_vector:
          filtered NumPy array, a list of numbers
        
          if the output_size would be less than desirable size, because of the
          filtering, the output is appended with zeros for now, will  be changed
          if needed
  '''
  from numpy import array
  
  output_vector = input_vector
  
  while len(output_vector) > size:
    output_vector = filter(output_vector, filter_size)
  
  while len(output_vector) < size:
    output_vector.append(0)
    
  return output_vector


def remove_elements(input = [], elements_set = []):
  '''
  Function takes two arguments:
    text:
        iterable variable from which the elements will be removed
    elements_set:
        iterable variable of elements to remove
    
  Function returns:
      NumPy array made of input, without the elements from elements_set
  '''
  from numpy import array
  
  return array([item for item in input if item not in set(elements_set)])


'''
    Loading texts from files. Starting with deceptive and following
    with truthful.
'''

path = 'drive/My Drive/data/transcription/'

# LOADING DECEPTIVE

file_names = [i for i in listdir(
    join(path,'deceptive/')) if isfile(join(path, 'deceptive/', i))]

dec_texts = []

for name in file_names:
    tmp = join(path, 'deceptive/', name)
    with open(tmp, 'r',  encoding="utf8") as f:
        dec_texts.append(f.read())

        

# LOADING TRUTHFUL
        
file_names = [i for i in listdir(
    join(path,'truthful/')) if isfile(join(path,'truthful/', i))]

tru_texts = []

for name in file_names:
    tmp = join(path, 'truthful/', name)
    with open(tmp, 'r',  encoding="utf8") as f:
        tru_texts.append(f.read())

"""**Creating pd.DataFrame to keep everything in one place and have some kind of backup.**"""

data = pd.DataFrame(tru_texts, columns = ['original_text'])
data['deceptive'] = 0
data['truthful'] = 1


data_f = pd.DataFrame(dec_texts, columns = ['original_text'])
data_f['deceptive'] = 1
data_f['truthful'] = 0

data = data.append(data_f)
data = data.reset_index(drop = True)

del(data_f)

# COUNTING TYPES OF SPEECH

data['edited_text'] = data['original_text'].apply(
    lambda x : text_preprocess(x))

data['tokens'] = data['edited_text'].apply(
    lambda x : wordpunct_tokenize(x))

data['parts_of_speech'] = data['edited_text'].apply(
    lambda x : (nltk.FreqDist(
        tag for (word, tag) in nltk.pos_tag(
            wordpunct_tokenize(x), tagset="universal"))))

data['parts_of_speech'] = data['parts_of_speech'].apply(
    lambda x : x.most_common(12))
data['sum_of_tokens'] = data['tokens'].apply(lambda x : len(x))

# verb, noun, adjective, adverb, determiner/article, pronoun
# adposition/preposition or postposition/, conjunction
# punctuation, unknown/other, numerical, particle
parts_of_speech = ['verb', 'noun', 'adj', 'adv', 'det',
                   'pron', 'adp', 'conj', 'x', 'num', 'prt']

for part in parts_of_speech:
  data[part] = data['parts_of_speech'].apply(
      lambda x : num_of_words(x, part.upper())[1])
  
  data[part] = data[part]/data['sum_of_tokens']

# TESTING FOR NORMALITY TO COMAPRE MEANS

column_names = data.columns

normally_distributed = []
non_normally_distributed = []

# setting up plots
num_of_hists = len(parts_of_speech)
num_of_rows = round(num_of_hists/2, 0)
num_of_columns = 2

fig = plt.figure(figsize=(14,14))

for i in range(7, len(column_names)):
  name = parts_of_speech[i-7]
  
  shapiro_test = shapiro(data[name])
  
  # fragment below prints histograms
  
  fig = plt.subplot(num_of_rows, num_of_columns, i-6)
  plt.title(name)
  plt.hist(data[data['truthful'] == 1][name], color = 'green', alpha = 0.5)
  plt.hist(data[data['deceptive'] == 1][name], color = 'red', alpha = 0.5)
  plt.subplots_adjust(bottom=0, top=2)
  
  if shapiro_test[1] < 0.05:
    non_normally_distributed.append(name)
  else:
    normally_distributed.append(name)

print()
print('Normally distributed: ', normally_distributed)
print('Non-normally distributed: ', non_normally_distributed)

print('t-tests for normally distributed parts of speech:\n')

for part in normally_distributed:
  print('{:<5}'.format(part),
        'p-value:',
        " {:.03f}".format(ttest(data[data['truthful'] == 1][part],
                                data[data['truthful'] == 0][part],
                                equal_var = False,
                                nan_policy = 'omit'
                               )[1]
                         )
       )

# null-hypothesis for the MW test is that the distributions are the same,
# so if p<0.05 reject the hypothesis

print('\nMann-Witney U-test for non-normally distributed parts of speech:\n')

for part in non_normally_distributed:
  print('{:<5}'.format(part),
        'p-value:',
        " {:.03f}".format(mwtest(data[data['truthful'] == 1][part],
                                 data[data['truthful'] == 0][part]
                                )[1]
                         )
       )

'''
  next steps for analysis:
    -> length of words
    -> most common words or expressions (combinations of 2-3 words) etc.
    -> number of negations, number of stopwords etc.
'''

def ave_len(tab = []):
  x = 0
  for item in tab:
    x += len(item)
  x = x/len(tab)
  return(x)

# MOST AND LEAST COMMON EXPRESSIONS

data['most_common'] = data['tokens'].apply(lambda x : nltk.FreqDist(x).most_common(10))

# TOKENS LENGHT

data['ave_token_len'] = data['tokens'].apply(lambda x : ave_len(x))

# t-test for length

plt.title('Average token length')
plt.hist(data[data['truthful'] == 1]['ave_token_len'], color = 'green', alpha = 0.5)
plt.hist(data[data['deceptive'] == 1]['ave_token_len'], color = 'red', alpha = 0.5)
plt.show()

print('Test for normal distribution:\n')
for i in [0,1]:
  shapiro_test = shapiro(data[data['deceptive'] == i]['ave_token_len'])
  print('For deceptive =', i, '   p-value = {:.3f}'.format(shapiro_test[1]))

print('\nTesting for equal means:\n\nt-test p-value: {:.3f}'.format(ttest(data[data['truthful'] == 1]['ave_token_len'], data[data['truthful'] == 0]['ave_token_len'], equal_var = False, nan_policy = 'omit')[1]))

"""**WORD VECTORIZATION**"""

'''
    Word vectorization model
'''

word2vec_model = gensim.models.Word2Vec(brown.sents())

'''
# IMPORTING GLOVE EMBEDDINGS TO DICT

glove_model = dict()

address = 'drive/My Drive/data/glove.6B.300d.txt'

with open(address, 'r', encoding="utf8") as f:
  for line in f:
      vector = line.split()
      word = vector[0]
      values = np.asarray(vector[1:], dtype='float32')
      glove_model[word] = values
'''


'''
    TEXT VECTORIZATION:
        -> model
        -> vectorization
        -> concatenation
'''

model_used = word2vec_model

stopword_set = set(stopwords.words('english'))

data['tokens_without_stopwords'] = data['tokens'].apply(
    lambda x : remove_elements(x, stopword_set))

data['vectors'] = data['tokens'].apply(
    lambda x : vect_and_concat(vecMod = model_used, input = x))
data['one_long_vec'] = data['vectors']
data['vectors'] = data['vectors'].apply(
    lambda x : np.split(x, len(x)/len(model_used['mother'])))

data['vectors_without_stopwords'] = data['tokens_without_stopwords'].apply(
    lambda x : vect_and_concat(vecMod = model_used, input = x))
data['vectors_without_stopwords'] = data['vectors_without_stopwords'].apply(
    lambda x : np.split(x, len(x)/len(model_used['mother'])))

'''
    IMPORTING THE MICROGESTURES VECTORS
'''

# READING DATA
gestures_df = pd.read_csv('drive/My Drive/data/annotation/gestures.csv')
gestures_df = gestures_df.dropna()
gestures_df = gestures_df.reset_index(drop=True)

tmpD = []
tmpT = []

for i in range(61):
  tmpD.append(1)
  tmpT.append(0)
for i in range(60):
  tmpD.append(0)
  tmpT.append(1)
  
gestures_df['deceptive'] = tmpD
gestures_df['truthful'] = tmpT

'''
    GENERATING TRAINING AND VALIDATION SETS (RANDOM SHUFFLE)
'''

gestures_df = gestures_df.sample(frac = 1).reset_index(drop = True)

train_set = gestures_df[:95]
test_set = gestures_df[96:]

x_train = np.array(train_set[train_set.columns[1:40]])
x_val = np.array(test_set[test_set.columns[1:40]])
y_train = np.array(train_set[train_set.columns[41:]])
y_val = np.array(test_set[test_set.columns[41:]])

'''
    BUILDING THE GESTURES-ONLY MODEL
    
    remarks:
      -> changing the activation of the final layer from 'linear' to 'relu'
         helps accuracy by approx. 0.06
'''

gestures_model = Sequential()
gestures_model.add(Dense(units = 1024, input_dim = 39, activation = 'relu'))
gestures_model.add(Dropout(0.5))
gestures_model.add(Dense(units = 1, activation = 'relu'))
gestures_model.compile(loss='binary_crossentropy',
                       optimizer='sgd',
                       metrics=['accuracy'])

# FITTING THE MODEL
gestures_model.fit(x_train,
                   y_train[:,0],
                   epochs = 10,
                   batch_size = 10,
                   verbose = 2
                  )


_, accuracy = gestures_model.evaluate(x = x_val,
                                      y = y_val[:,0],
                                      batch_size = 10,
                                      verbose = 0
                                     )

print('\nGestures-only model accuracy: {:.3f}'.format(accuracy))

gestures_model.summary()

'''
    SVM model for gestures
'''

nfolds = 10
parameters = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10],
              'gamma' : [0.001, 0.01, 0.1, 1.0],
              'kernel' : ['rbf', 'linear', 'poly', 'sigmoid'],
              'coef0' : [-0.4, -0.3, -0.2, 0, 0.1, 0.2, 0.3, 0.4],
              'shrinking' : [True, False],
              'decision_function_shape' : ['ovo', 'ovr']
             } 
gestures_svm = GridSearchCV(svm.SVC(random_state = 21031994),
                            parameters, cv = nfolds)
gestures_svm.fit(x_train, y_train[:,0])

y_predict = gestures_svm.predict(x_val)
print('Accuracy for the SVM model:', accuracy_score(y_val[:,0], y_predict))

'''
    LOGISTIC regression model for gestures
'''

gestures_LogReg = LogisticRegression(random_state = 21031994,
                                     solver='lbfgs',
                                     multi_class='multinomial'
                                    )

gestures_LogReg.fit(x_train, y_train[:,0])

y_predict = gestures_LogReg.predict(x_val)
print('Accuracy for the LogReg model:', accuracy_score(y_val[:,0], y_predict))
print('Average score: {:.2}'.format(cross_val_score(
    gestures_LogReg,
    np.concatenate((x_train, x_val), axis=0),
    np.concatenate((y_train[:,0], y_val[:,0]),axis = 0), cv=10).mean())
     )

'''
    LINEAR regression model for gestures
'''

gestures_LinReg = LinearRegression()
gestures_LinReg.fit(x_train, y_train[:,0])

y_predict = gestures_LinReg.predict(x_val)
print('R^2 for the LingReg model: {:.3f}'.format(
    gestures_LinReg.score(x_val, y_val[:,0])))


'''
    NEURAL NETWORK IMPLEMENTATION:
        
    FURTHER PREPROCESSING:
        -> lemmatizing
        -> vectorizing the tokens
        -> filtering the input until it meets certain size criteria
'''

'''
    Selecting training and test set for model building:
    
    -> for now it's mostly random, CV to be implemented in all cases
'''

df = data[['vectors', 'deceptive']]

train_df = df[df['deceptive']==1][:45].append(df[df['deceptive']==0][:45],
                                              ignore_index=True
                                             )
train_df = shuffle(train_df).reset_index(drop = 'True')
train_df['length'] = train_df['vectors'].apply(lambda x : len(x))
train_df['simple'] = train_df['vectors'].apply(lambda x : np.average(x,
                                                                     axis = 0
                                                                    )
                                              )

test_df = df[df['deceptive']==1][45:].append(df[df['deceptive']==0][45:],
                                             ignore_index=True
                                            )
test_df = shuffle(test_df).reset_index(drop = 'True')
test_df['length'] = test_df['vectors'].apply(lambda x : len(x))
test_df['simple'] = test_df['vectors'].apply(lambda x : np.average(x, axis = 0))

print(data[['vectors', 'deceptive']].count(), '\n')
print(train_df.count(), '\n')
print(test_df.count(), '\n')

'''
    TRANSCRIPT-ONLY MODEL WITH CONVOLUTIONAL LAYER
'''

x_train = np.array(train_df['simple'])
x_train = np.concatenate(x_train, axis = None).reshape(
    (90,len(word2vec_model['mother']), 1))
y_train = np.array(train_df['deceptive'])

x_val = np.array(test_df['simple'])
x_val = np.concatenate(x_val, axis = None).reshape((31, 100, 1))
y_val = np.array(test_df['deceptive'])

x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
y_train = y_train.reshape(y_train.shape[0], 1)

cnn_model = Sequential()
cnn_model.add(Conv1D(input_shape = (100, 1), filters = 3, kernel_size = 2))
cnn_model.add(MaxPooling1D(pool_size = 2))
cnn_model.add(Flatten())
cnn_model.add(Dense(300, activation = 'relu'))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(1, activation = 'sigmoid'))
cnn_model.compile(loss = 'binary_crossentropy', optimizer = 'sgd',
                  metrics = ['accuracy'])

# FITTING THE MODEL
cnn_model.fit(x = x_train,
              y = y_train,
              epochs = 10,
              batch_size = 10,
              verbose = 2)

# EVALUATING THE MODEL
_, accuracy = cnn_model.evaluate(x_val, y_val, batch_size = 10, verbose = 2)
print('\nAccuracy: {:.2}'.format(accuracy))

# MODEL SUMMARY
cnn_model.summary()
